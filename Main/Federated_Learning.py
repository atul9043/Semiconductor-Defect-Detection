class FederatedWaferLearning:
    """FIXED: Federated learning with better error handling"""

    def __init__(self, num_clients, num_classes):
        self.num_clients = num_clients
        self.num_classes = num_classes
        self.global_model = None
        self.client_models = {}
        self.xai_analyzer = None
        self.round_history = []

    def initialize_global_model(self, input_shape):
        """Initialize the global model"""
        self.global_model = create_federated_mobilenetv2_model(input_shape, self.num_classes)
        self.xai_analyzer = WaferXAIAnalyzer(self.global_model)
        print(f"Global model initialized with {self.global_model.count_params():,} parameters")

    def federated_averaging(self, client_weights, client_sizes):
        """Perform FedAvg aggregation"""
        if not client_weights:
            return self.global_model.get_weights()

        total_size = sum(client_sizes)
        if total_size == 0:
            return self.global_model.get_weights()

        aggregation_weights = [size / total_size for size in client_sizes]
        global_weights = self.global_model.get_weights()
        aggregated_weights = [np.zeros_like(w) for w in global_weights]

        for client_idx, (weights, agg_weight) in enumerate(zip(client_weights, aggregation_weights)):
            for i, layer_weights in enumerate(weights):
                aggregated_weights[i] += layer_weights * agg_weight

        return aggregated_weights

    def train_federated_round(self, clients_data, val_data, round_num,
                            local_epochs=2, xai_feedback=False):
        """FIXED: Execute federated round with better data handling"""
        print(f"\n{'='*60}")
        print(f"FEDERATED ROUND {round_num}")
        print(f"{'='*60}")

        client_weights = []
        client_sizes = []
        client_metrics = {}

        for client_id in range(self.num_clients):
            client_name = f"client_{client_id}"
            if client_name not in clients_data:
                print(f"Skipping {client_name} - no data")
                continue

            print(f"\nTraining {client_name}...")

            # Create client model
            client_model = create_federated_mobilenetv2_model(
                input_shape=(224, 224, 3),
                num_classes=self.num_classes
            )
            client_model.set_weights(self.global_model.get_weights())

            # Get client data
            client_data = clients_data[client_name]

            # Count samples (more robust method)
            try:
                client_size = sum(1 for _ in client_data.take(1000))  # Limit counting
            except:
                client_size = 100  # Default estimate

            client_sizes.append(client_size)

            # Local training with error handling
            try:
                history = client_model.fit(
                    client_data,
                    epochs=local_epochs,
                    verbose=0,
                    validation_data=val_data.take(5) if round_num % 5 == 0 else None
                )

                client_weights.append(client_model.get_weights())

                # XAI Analysis
                if xai_feedback and XAI_AVAILABLE:
                    try:
                        xai_analyzer = WaferXAIAnalyzer(client_model)
                        focus_metrics = xai_analyzer.analyze_model_focus(client_data)
                        client_metrics[client_name] = {
                            'loss': history.history['loss'][-1],
                            'accuracy': history.history['accuracy'][-1],
                            'focus_score': focus_metrics['focus_score'],
                            'defect_coverage': focus_metrics['defect_coverage']
                        }
                        print(f"  {client_name} - Acc: {history.history['accuracy'][-1]:.3f}, "
                              f"Focus: {focus_metrics['focus_score']:.3f}")
                    except Exception as e:
                        print(f"  XAI analysis failed for {client_name}: {e}")
                        client_metrics[client_name] = {
                            'loss': history.history['loss'][-1],
                            'accuracy': history.history['accuracy'][-1]
                        }
                else:
                    client_metrics[client_name] = {
                        'loss': history.history['loss'][-1],
                        'accuracy': history.history['accuracy'][-1]
                    }
                    print(f"  {client_name} - Acc: {history.history['accuracy'][-1]:.3f}")

            except Exception as e:
                print(f"  Training failed for {client_name}: {e}")
                continue

        # Federated averaging
        if client_weights:
            print(f"\nAggregating {len(client_weights)} client models...")
            aggregated_weights = self.federated_averaging(client_weights, client_sizes)
            self.global_model.set_weights(aggregated_weights)
        else:
            print("‚ö†Ô∏è No client weights to aggregate!")

        # Evaluate global model
        try:
            print("Evaluating global model...")
            global_metrics = self.global_model.evaluate(val_data.take(10), verbose=0)
        except Exception as e:
            print(f"Global evaluation failed: {e}")
            global_metrics = [1.0, 0.5, 0.7]  # Default values

        round_results = {
            'round': round_num,
            'global_loss': global_metrics[0],
            'global_accuracy': global_metrics[1],
            'client_metrics': client_metrics,
            'num_clients': len(client_weights)
        }

        self.round_history.append(round_results)

        print(f"Global Model - Loss: {global_metrics[0]:.4f}, "
              f"Accuracy: {global_metrics[1]:.4f} ({global_metrics[1]*100:.2f}%)")

        return round_results

    def run_federated_learning(self, clients_data, val_data, test_data,
                              num_rounds=10, local_epochs=2, xai_feedback=True):
        """FIXED: Run complete federated learning process"""
        print(f"\n{'='*80}")
        print(f"STARTING FEDERATED LEARNING WITH XAI FEEDBACK")
        print(f"Clients: {len(clients_data)}, Rounds: {num_rounds}, Local Epochs: {local_epochs}")
        print(f"XAI Feedback: {xai_feedback and XAI_AVAILABLE}")
        print(f"{'='*80}")

        # Initialize global model
        self.initialize_global_model((224, 224, 3))

        # Run federated rounds
        best_accuracy = 0.0
        for round_num in range(1, num_rounds + 1):
            try:
                round_results = self.train_federated_round(
                    clients_data, val_data, round_num,
                    local_epochs, xai_feedback
                )

                # Track best accuracy
                current_acc = round_results['global_accuracy']
                if current_acc > best_accuracy:
                    best_accuracy = current_acc
                    print(f"üéØ New best accuracy: {best_accuracy:.4f}")

                # Early stopping
                if current_acc > 0.85:
                    print(f"\nüéØ Target accuracy achieved at round {round_num}!")
                    break

            except Exception as e:
                print(f"Round {round_num} failed: {e}")
                continue

        # Final evaluation
        print(f"\n{'='*80}")
        print("FINAL EVALUATION")
        print(f"{'='*80}")

        try:
            final_results = self.global_model.evaluate(test_data, verbose=1)

            print(f"\nüèÜ FEDERATED LEARNING RESULTS:")
            print(f"   Final Test Loss: {final_results[0]:.4f}")
            print(f"   Final Test Accuracy: {final_results[1]:.4f} ({final_results[1]*100:.2f}%)")
            if len(final_results) > 2:
                print(f"   Final Test Top-3 Accuracy: {final_results[2]:.4f} ({final_results[2]*100:.2f}%)")

            if final_results[1] > 0.80:
                print("‚úÖ Excellent federated performance achieved!")
            elif final_results[1] > 0.70:
                print("‚úÖ Good federated performance achieved!")
            else:
                print("‚ö†Ô∏è Consider more rounds or hyperparameter tuning")

        except Exception as e:
            print(f"Final evaluation failed: {e}")

        return self.global_model, self.round_history
